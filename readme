"""📌 Basic Explanation of the RAG Model in This Implementation
This Retrieval-Augmented Generation (RAG) model is designed to process dialysis patient data from a CSV file and allow users to query the data using a chatbot interface.

🔹 How It Works
1️⃣ Data Loading: The system reads and processes the CSV file using pandas.
2️⃣ Vector Database Creation (FAISS): The text data is converted into embeddings and stored in a FAISS index for fast retrieval.
3️⃣ Query Handling: When a user asks a question, the model retrieves relevant documents from FAISS.
4️⃣ Response Generation (LLM): The retrieved information is passed to an LLM (Large Language Model) to generate a natural language answer.
5️⃣ Chatbot UI (Streamlit): A user-friendly chatbot interface is built using Streamlit, allowing conversational interaction with the dataset.

📌 Technologies & Models Used
Below are the key technologies and models used in the implementation:

1️⃣ FAISS (Facebook AI Similarity Search)
📌 What is FAISS?

FAISS is an efficient vector database used for fast retrieval of similar text chunks.
It allows quick similarity searches in large datasets.
📌 Why is FAISS used here?

Instead of searching raw text in a CSV file, we convert text into vectors and store them in FAISS, making queries much faster.
📌 How FAISS works in this project?

Step 1: The CSV data is converted into text chunks.
Step 2: These chunks are embedded using a sentence transformer (all-MiniLM-L6-v2).
Step 3: FAISS stores and retrieves the most relevant chunks based on similarity.
2️⃣ SentenceTransformer (all-MiniLM-L6-v2)
📌 What is this model?

all-MiniLM-L6-v2 is a sentence embedding model from Hugging Face.
It converts text into vector representations, which helps in similarity searches.
📌 Why are embeddings needed?

To compare text effectively, we need a numerical representation.
Example: If a user asks, "What is the body temperature of Patient 2?", we find similar text chunks instead of doing an exact word match.
📌 Why use all-MiniLM-L6-v2 instead of other models?

Faster and lightweight ✅
Good accuracy for sentence similarity ✅
Works well with FAISS ✅
3️⃣ LLM (Large Language Model) – Moondream
📌 What is an LLM?

An LLM is an AI model trained to generate human-like text.
It takes retrieved text and generates a meaningful answer.
📌 Why use moondream LLM?

It’s optimized for answering queries based on context.
It integrates well with FAISS and Streamlit.
📌 How does the LLM work here?

Retrieves relevant documents from FAISS.
Uses the retrieved context to generate a natural response.
Returns the final answer to the user.
4️⃣ Streamlit (For Chatbot UI)
📌 Why use Streamlit?

It’s a lightweight framework for building interactive web apps.
Perfect for AI and ML models that require a simple chatbot-like UI.
📌 What does Streamlit do in this project?

File Upload: Allows users to upload a CSV file.
Chat Interface: Displays user queries & responses.
Real-time Processing: Shows retrieved documents and their sources.
📌 Overall Flow of the Model
1️⃣ User uploads a CSV file → The data is loaded.
2️⃣ FAISS creates a vector store → Data is split into chunks and embedded.
3️⃣ User enters a query → FAISS retrieves the most relevant text.
4️⃣ LLM generates an answer → The retrieved context is used to generate a response.
5️⃣ Chatbot displays the answer → The result is shown to the user.

🔹 Why is This Model Powerful?
✅ Faster than traditional text searches (Uses FAISS for rapid retrieval).
✅ Generates human-like responses (Uses an LLM instead of keyword matching).
✅ Works on any dataset (CSV file can contain any structured data).
✅ User-friendly UI (Built using Streamlit for easy interaction).

This RAG-based chatbot allows fast and interactive querying of patient data, making it ideal for medical data analysis, business intelligence, or any structured document search. 🚀

#implementation

import os
import streamlit as st
import pandas as pd
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# FAISS index path
FAISS_INDEX_PATH = "faiss_index"

# 1️⃣ **Load & Process Data Efficiently**
@st.cache_resource
def create_vector_db(data_path):  
    """Loads CSV, formats data into structured text, and creates FAISS vectorstore."""
    
    df = pd.read_csv(data_path)

    # ✅ Convert dataset into structured text (AI understands this better)
    records = df.to_dict(orient="records")
    documents = [
        f"Patient {rec['Patient_ID']} ({rec['Sex']}, {rec['Age']} years old)\n"
        f"- Date: {rec['Datetime']}\n"
        f"- Hypertension: {'Yes' if rec['Is_Hypertensive'] else 'No'}\n"
        f"- Diabetes: {'Yes' if rec['Is_Diabetic'] else 'No'}\n"
        f"- Dialyzer: {rec['Dialyzer']}, Technique: {rec['Type_of_Technique']}\n"
        f"- Body Temperature: {rec['Body_Temperature']}°C, Heart Rate: {rec['Heart_Rate']} bpm\n"
        f"- Blood Pressure: {rec['Systolic_BP']}/{rec['Diastolic_BP']} mmHg\n"
        f"- Urea Clearance: {rec['Urea_Clearance']}, Volume Changes: {rec['Volume_Changes']}\n"
        "------------------------------------------"
        for rec in records
    ]
    
    # ✅ Ensure text is split properly for FAISS
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.create_documents(documents)

    # ✅ Use Efficient Embedding Model
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

    # ✅ Load or Create FAISS Index
    if os.path.exists(FAISS_INDEX_PATH):  
        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        db = FAISS.from_documents(chunks, embeddings)
        db.save_local(FAISS_INDEX_PATH)  
    
    return db

# 2️⃣ **Setup LLM & Prompt for Retrieval-Based QA**
ollama_llm = OllamaLLM(base_url="http://localhost:11434", model="moondream")

# ✅ **Improved Prompt for Better AI Answers**
PROMPT_TEMPLATE = PromptTemplate(
    input_variables=["context", "question"],
    template="""You are an AI trained to analyze dialysis patient records.
    
    Given the following patient data:
    {context}
    
    Extract the precise answer from the dataset.
    
    Question: {question}
    
    If the answer is not found, respond with: "I could not find that information in the dataset."
    """
)

# 3️⃣ **Streamlit UI: Chatbot**
st.set_page_config(page_title="Dialysis Data Chatbot", page_icon="🤖", layout="wide")
st.title("💬 Dialysis Data Chatbot")

st.sidebar.title("⚙️ Upload Data")
uploaded_file = st.sidebar.file_uploader("📂 Upload CSV File", type="csv")

if uploaded_file:
    db = create_vector_db(uploaded_file.name)
    retriever = db.as_retriever()

    # 🔹 **Chat Interface**
    st.subheader("💡 Chat with the Data")
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    input_txt = st.text_input("🧐 Ask something about the data...")

    if st.button("🔍 Get Answer"):
        if input_txt:
            with st.spinner("🔎 Searching..."):
                retrieved_docs = retriever.get_relevant_documents(input_txt)

                if not retrieved_docs:
                    response = "⚠️ I could not find that information in the dataset."
                else:
                    # ✅ **Pass structured context to the AI**
                    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # ✅ **Use Improved Prompt to Ensure Correct Answer Extraction**
                    qa_chain = RetrievalQA.from_chain_type(
                        llm=ollama_llm,
                        retriever=retriever,
                        chain_type="stuff",
                        return_source_documents=True,
                        chain_type_kwargs={"prompt": PROMPT_TEMPLATE}
                    )
                    result = qa_chain({"query": input_txt, "context": context_text})
                    response = result['result']

                # 🔹 **Update Chat History**
                st.session_state.chat_history.append(("User", input_txt))
                st.session_state.chat_history.append(("AI", response))

                # 🔹 **Display Chat**
                for sender, msg in st.session_state.chat_history:
                    if sender == "User":
                        st.markdown(f"**🧑‍💻 {sender}:** {msg}")
                    else:
                        st.markdown(f"**🤖 {sender}:** {msg}")

else:
    st.info("📂 Please upload a CSV file to start.")"""